{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6343ac3a",
   "metadata": {},
   "source": [
    "# Taking Video Live stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb60d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c951e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Display the frame\n",
    "        cv2.imshow('Live Stream', frame)\n",
    "        \n",
    "        # Check for 'q' key press to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04290c",
   "metadata": {},
   "source": [
    "# Adding feature to video capture that can draw lines by tracking lines hand and fingers using media pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python mediapipe numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdece12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480\n",
    "IMAGE_SIZE = (48, 48)\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if gesture_detection:\n",
    "            # Process the frame with MediaPipe Hands\n",
    "            results = hands.process(frame_rgb)\n",
    "\n",
    "            # Check if hand landmarks are detected\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Extract hand landmarks and visualize them\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Get index finger tip coordinates\n",
    "                    index_finger_coords = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    x, y = int(index_finger_coords.x * frame.shape[1]), int(index_finger_coords.y * frame.shape[0])\n",
    "\n",
    "                    if not drawing:\n",
    "                        # Create a new canvas if not already drawing\n",
    "                        canvas = np.zeros_like(frame)\n",
    "\n",
    "                    # Start drawing\n",
    "                    drawing = True\n",
    "\n",
    "                    # Draw line on the canvas\n",
    "                    if start_point is not None:\n",
    "                        cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "                    start_point = (x, y)\n",
    "\n",
    "            else:\n",
    "                # Stop drawing if no hand is detected\n",
    "                drawing = False\n",
    "                start_point = None\n",
    "        else:\n",
    "            # Drawing mode, no hand gesture detection\n",
    "\n",
    "            # Start drawing\n",
    "            if start_point is None:\n",
    "                start_point = (0, 0)\n",
    "\n",
    "            # Get index finger tip coordinates based on mouse movement\n",
    "            x, y = start_point\n",
    "\n",
    "            # Draw line on the canvas\n",
    "            if drawing:\n",
    "                cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "            start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "        \n",
    "        \n",
    "        # Save the drawn image on the blank canvas\n",
    "        blank_canvas[:, :int(WINDOW_WIDTH / 2)] = canvas\n",
    "\n",
    "        # Convert the drawn image to grayscale\n",
    "        grayscale_canvas = cv2.cvtColor(blank_canvas, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Perform any additional operations on the grayscale canvas as needed\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Save the grayscale canvas\n",
    "        cv2.imwrite('drawn_image.jpg', grayscale_canvas)\n",
    "\n",
    "                # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "                # Save the drawn image when 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            # Save the grayscale canvas\n",
    "            cv2.imwrite('drawn_image.jpg', grayscale_canvas)\n",
    "            print(\"Image saved as 'drawn_image.jpg'\")\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "\n",
    "        # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros_like(frame)\n",
    "            print(\"Cleared canvas\")\n",
    "\n",
    "        # Change draw color to red if '1' key is pressed\n",
    "        if key == ord('1'):\n",
    "            draw_color = (0, 0, 255)  # Red\n",
    "            print(\"Switched to red color\")\n",
    "\n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('2'):\n",
    "            draw_color = (0, 255, 0)  # Green\n",
    "            print(\"Switched to green color\")\n",
    "\n",
    "        # Change draw color to blue if '3' key is pressed\n",
    "        if key == ord('3'):\n",
    "            draw_color = (255, 0, 0)  # Blue\n",
    "            print(\"Switched to blue color\")\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a5d8f",
   "metadata": {},
   "source": [
    "### challenges\n",
    "One challenge that we faced was hardware issues while using MediaPipe for hand gesture detection, resulting in output drawings appearing as dotted lines. To overcome this, we implemented the traceback method to get continuous line images. This method likely involved modifying or optimizing the hand gesture detection algorithm to ensure a smooth tracking of hand movements, leading to better results in drawing continuous lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf7069",
   "metadata": {},
   "source": [
    "# Hand Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662badcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Check if hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Draw lines connecting the landmarks for better visualization of hand movement\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                          mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=4),\n",
    "                                          mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                                          )\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Hand Gesture Detection', frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        if gesture_detection:\n",
    "            # Gesture detection mode\n",
    "\n",
    "            # Check if hand landmarks are detected\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Draw landmarks on the frame\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                              mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                                              mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)\n",
    "                                              )\n",
    "\n",
    "        else:\n",
    "            # Drawing mode, no hand gesture detection\n",
    "\n",
    "            # Get index finger tip coordinates based on mouse movement\n",
    "            if start_point is None:\n",
    "                x, y = 0, 0\n",
    "            else:\n",
    "                x, y = start_point\n",
    "\n",
    "            # Draw line on the canvas\n",
    "            if drawing:\n",
    "                cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "            start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "                    # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Start drawing if 'd' key is pressed\n",
    "        if key == ord('d'):\n",
    "            drawing = True\n",
    "\n",
    "        # Stop drawing if 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            drawing = False\n",
    "\n",
    "# Release the video capture and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170382c0",
   "metadata": {},
   "source": [
    "### challenges\n",
    " One of the challenges that we faced in this code is achieving real-time performance for both hand gesture detection and drawing on the canvas. The continuous processing of video frames and the integration of mediapipe, hands for gesture detection while simultaneously updating the canvas in drawing mode can cause performance bottlenecks. To overcome this, we did efficient asynchronous processing techniques this way the hand gesture detection and drawing processes can be executed concurrently, ensuring a smooth and responsive user experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6f095",
   "metadata": {},
   "source": [
    "### Showing Landmarks in Gesture mode and able to draw in Drawing mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Check if hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract hand landmarks and visualize them\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if gesture_detection:\n",
    "                    # Get bounding box coordinates of the hand\n",
    "                    landmarks = hand_landmarks.landmark\n",
    "                    x_values = [landmark.x for landmark in landmarks]\n",
    "                    y_values = [landmark.y for landmark in landmarks]\n",
    "                    min_x = min(x_values) * frame.shape[1]\n",
    "                    max_x = max(x_values) * frame.shape[1]\n",
    "                    min_y = min(y_values) * frame.shape[0]\n",
    "                    max_y = max(y_values) * frame.shape[0]\n",
    "\n",
    "                    # Draw a green rectangle around the hand\n",
    "                    cv2.rectangle(frame, (int(min_x), int(min_y)), (int(max_x), int(max_y)), (0, 255, 0), 2)\n",
    "\n",
    "                    # Start drawing\n",
    "                    if drawing:\n",
    "                        # Get index finger tip coordinates\n",
    "                        index_finger_coords = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                        x, y = int(index_finger_coords.x * frame.shape[1]), int(index_finger_coords.y * frame.shape[0])\n",
    "\n",
    "                        # Draw line on the canvas\n",
    "                        cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "                    start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "\n",
    "        # Start drawing if 'd' key is pressed\n",
    "        if key == ord('d'):\n",
    "            drawing = True\n",
    "            print(\"Started drawing\")\n",
    "\n",
    "        # Stop drawing if 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            drawing = False\n",
    "            print(\"Stopped drawing\")\n",
    "\n",
    "        # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros_like(frame)\n",
    "            print(\"Cleared canvas\")\n",
    "\n",
    "        # Change draw color to red if '1' key is pressed\n",
    "        if key == ord('1'):\n",
    "            draw_color = (0, 0, 255)  # Red\n",
    "            print(\"Switched to red color\")\n",
    "\n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('2'):\n",
    "            draw_color = (0, 255, 0)  # Green\n",
    "            print(\"Switched to green color\")\n",
    "\n",
    "        # Change draw color to blue if '3' key is pressed\n",
    "        if key == ord('3'):\n",
    "            draw_color = (255, 0, 0)  # Blue\n",
    "            print(\"Switched to blue color\")\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9263f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode\n",
    "\n",
    "def save_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the image to (48, 48) dimension\n",
    "    resized = cv2.resize(gray, (48, 48))\n",
    "\n",
    "    # Save the image\n",
    "    cv2.imwrite(\"drawn_image.jpg\", resized)\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Check if hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract hand landmarks and visualize them\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if gesture_detection:\n",
    "                    # Get bounding box coordinates of the hand\n",
    "                    landmarks = hand_landmarks.landmark\n",
    "                    x_values = [landmark.x for landmark in landmarks]\n",
    "                    y_values = [landmark.y for landmark in landmarks]\n",
    "                    min_x = min(x_values) * frame.shape[1]\n",
    "                    max_x = max(x_values) * frame.shape[1]\n",
    "                    min_y = min(y_values) * frame.shape[0]\n",
    "                    max_y = max(y_values) * frame.shape[0]\n",
    "\n",
    "                    # Draw a green rectangle around the hand\n",
    "                    cv2.rectangle(frame, (int(min_x), int(min_y)), (int(max_x), int(max_y)), (0, 255, 0), 2)\n",
    "\n",
    "                    # Start drawing\n",
    "                    if drawing:\n",
    "                        # Get index finger tip coordinates\n",
    "                        index_finger_coords = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                        x, y = int(index_finger_coords.x * frame.shape[1]), int(index_finger_coords.y * frame.shape[0])\n",
    "\n",
    "                        # Draw line on the canvas\n",
    "                        cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "                    start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "\n",
    "        # Start drawing if 'd' key is pressed\n",
    "        if key == ord('d'):\n",
    "            drawing = True\n",
    "            print(\"Started drawing\")\n",
    "\n",
    "        # Stop drawing if 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            drawing = False\n",
    "            print(\"Stopped drawing\")\n",
    "            save_image(canvas)\n",
    "\n",
    "        # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros_like(frame)\n",
    "            print(\"Cleared canvas\")\n",
    "\n",
    "        # Change draw color to red if '1' key is pressed\n",
    "        if key == ord('1'):\n",
    "            draw_color = (0, 0, 255)  # Red\n",
    "            print(\"Switched to red color\")\n",
    "\n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('2'):\n",
    "            draw_color = (0, 255, 0)  # Green\n",
    "            print(\"Switched to green color\")\n",
    "\n",
    "        # Change draw color to blue if '3' key is pressed\n",
    "        if key == ord('3'):\n",
    "            draw_color = (255, 0, 0)  # Blue\n",
    "            print(\"Switched to blue color\")\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a911271d",
   "metadata": {},
   "source": [
    "### Comment:\n",
    "Two additional challenges we encountered were enhancing the image saving functionality and expanding the color selection options that is we have introduced functionality to save the drawn image by pressing the 's' key. However, in the current implementation, the image is overwritten each time the user presses 's'. To enhance this feature, we did add options to save new drawings to existing images. And also we defined three predefined colors(red, green, and blue) for drawing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8841dae",
   "metadata": {},
   "source": [
    "# Model that draws and makes predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5973585",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36464a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af202282",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bcdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310152ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5016b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdabd\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44010e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from tensorflow import keras\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a42d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e40e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441b4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2825f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431e852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e263bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8edc97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained VGG16-based model\n",
    "model = keras.models.load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e11e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Convert the image to 8-bit per channel or 32-bit float\n",
    "    if image.dtype == np.float64:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "    elif image.dtype == np.float32:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Resize the image to match the input size of the model\n",
    "    image = resize(image, (48, 48), anti_aliasing=True)\n",
    "\n",
    "    # Check if the image is grayscale\n",
    "    if len(image.shape) == 2:\n",
    "        # Convert the grayscale image to RGB using PIL\n",
    "        image = np.stack((image,) * 3, axis=-1)\n",
    "        image = Image.fromarray(image.squeeze(), mode='RGB')\n",
    "    elif image.shape[0] == 1 and image.shape[1] == 1:\n",
    "        # Handle single-pixel image\n",
    "        image = np.tile(image, (48, 48, 3))\n",
    "        image = Image.fromarray(image.squeeze(), mode='RGB')\n",
    "    else:\n",
    "        # Convert the image to RGB using PIL\n",
    "        image = Image.fromarray(image, mode='RGB')\n",
    "\n",
    "    # Convert the image to numpy array\n",
    "    image = np.array(image)\n",
    "\n",
    "    # Normalize the pixel values\n",
    "    image = image.astype('float32') / 255.0\n",
    "\n",
    "    # Reshape the image to match the input shape of the model\n",
    "    image = np.reshape(image, (1, 48, 48, 3))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e2ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the image to (48, 48) dimension\n",
    "    resized = cv2.resize(gray, (48, 48))\n",
    "\n",
    "    # Save the image\n",
    "    cv2.imwrite(\"drawn_image.jpg\", resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2c55565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input image\n",
    "def preprocess_image(image):\n",
    "    # Resize the image to match the input shape of the model (48x48)\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    # Convert the image to grayscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Convert the grayscale image to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    # Reshape the image to match the input shape of the model (add batch dimension)\n",
    "    image_rgb = np.expand_dims(image_rgb, axis=0)\n",
    "    # Normalize the pixel values\n",
    "    image_rgb = image_rgb.astype('float32') / 255.0\n",
    "    return image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a991fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started drawing\n",
      "Switched to green color\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "Predicted Number: 1\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Predicted Number: 1\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted Number: 5\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Predicted Number: 1\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Number: 6\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Predicted Number: 8\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 8\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Predicted Number: 6\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Predicted Number: 6\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted Number: 6\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Check if hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract hand landmarks and visualize them\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if gesture_detection:\n",
    "                    # Get bounding box coordinates of the hand\n",
    "                    landmarks = hand_landmarks.landmark\n",
    "                    x_values = [landmark.x for landmark in landmarks]\n",
    "                    y_values = [landmark.y for landmark in landmarks]\n",
    "                    min_x = min(x_values) * frame.shape[1]\n",
    "                    max_x = max(x_values) * frame.shape[1]\n",
    "                    min_y = min(y_values) * frame.shape[0]\n",
    "                    max_y = max(y_values) * frame.shape[0]\n",
    "\n",
    "                    # Draw a green rectangle around the hand\n",
    "                    cv2.rectangle(frame, (int(min_x), int(min_y)), (int(max_x), int(max_y)), (0, 255, 0), 2)\n",
    "\n",
    "                    # Start drawing\n",
    "                    if drawing:\n",
    "                        # Get index finger tip coordinates\n",
    "                        index_finger_coords = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                        x, y = int(index_finger_coords.x * frame.shape[1]), int(index_finger_coords.y * frame.shape[0])\n",
    "\n",
    "                        # Draw line on the canvas\n",
    "                        cv2.line(canvas, start_point, (x, y), draw_color, thickness=20)\n",
    "                        start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "\n",
    "        # Start drawing if 'd' key is pressed\n",
    "        if key == ord('d'):\n",
    "            drawing = True\n",
    "            print(\"Started drawing\")\n",
    "\n",
    "        # Stop drawing if 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            drawing = False\n",
    "            print(\"Stopped drawing\")\n",
    "            save_image(canvas)\n",
    "\n",
    "        # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros_like(frame)\n",
    "            print(\"Cleared canvas\")\n",
    "\n",
    "                # Change draw color to red if '1' key is pressed\n",
    "        if key == ord('1'):\n",
    "            draw_color = (0, 0, 255)  # Red\n",
    "            print(\"Switched to red color\")\n",
    "            \n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('0'):\n",
    "            draw_color = (255, 255, 255)  # Green\n",
    "            print(\"Switched to white color\")\n",
    "\n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('2'):\n",
    "            draw_color = (0, 255, 0)  # Green\n",
    "            print(\"Switched to green color\")\n",
    "\n",
    "        # Change draw color to blue if '3' key is pressed\n",
    "        if key == ord('3'):\n",
    "            draw_color = (255, 0, 0)  # Blue\n",
    "            print(\"Switched to blue color\")\n",
    "\n",
    "        # Perform prediction if 'g' key is pressed\n",
    "        if key == ord('g'):\n",
    "        # Load and preprocess the input image\n",
    "            image_path = 'drawn_image.jpg'\n",
    "            input_image = cv2.imread(image_path)\n",
    "            preprocessed_image = preprocess_image(input_image)\n",
    "\n",
    "            # Make a prediction\n",
    "            prediction = model.predict(preprocessed_image)\n",
    "            predicted_class = np.argmax(prediction)\n",
    "\n",
    "            # Convert the predicted class to text\n",
    "            predicted_text = str(predicted_class)\n",
    "            print(\"Predicted Number:\", predicted_text)\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0b75d",
   "metadata": {},
   "source": [
    "### Challenges:\n",
    "One challenge was to correctly preprocess the drawn image so that it could be fed into the trained model for prediction. The function preprocess_image was introduced to resize and normalize the image to match the input size and data format expected by the model. we also included an option to perform real-time prediction by pressing the 'g' key. However, successfully integrating the prediction process into the main loop while maintaining a smooth user experience required careful handling. To achieve this, we first saved the drawn image using the save_image function, then loaded and preprocessed the saved image for prediction. After making a prediction using the trained model, we obtained the predicted class and converted it to text for display. Ensuring proper synchronization of image saving and prediction within the real-time loop was essential to achieve smooth and accurate predictions without impacting the overall user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a0785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6753e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
