{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6343ac3a",
   "metadata": {},
   "source": [
    "# Taking Video Live stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c951e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Display the frame\n",
    "        cv2.imshow('Live Stream', frame)\n",
    "        \n",
    "        # Check for 'q' key press to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04290c",
   "metadata": {},
   "source": [
    "# Adding feature to video capture that can draw lines by tracking lines hand and fingers using media pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49bb3afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\moham\\appdata\\roaming\\python\\python39\\site-packages (4.7.0.68)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\moham\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\moham\\anaconda3\\lib\\site-packages (1.25.0)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mediapipe) (3.5.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mediapipe) (23.1.4)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (22.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\moham\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\moham\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python mediapipe numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdece12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as 'drawn_image.jpg'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480\n",
    "IMAGE_SIZE = (48, 48)\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if gesture_detection:\n",
    "            # Process the frame with MediaPipe Hands\n",
    "            results = hands.process(frame_rgb)\n",
    "\n",
    "            # Check if hand landmarks are detected\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Extract hand landmarks and visualize them\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Get index finger tip coordinates\n",
    "                    index_finger_coords = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                    x, y = int(index_finger_coords.x * frame.shape[1]), int(index_finger_coords.y * frame.shape[0])\n",
    "\n",
    "                    if not drawing:\n",
    "                        # Create a new canvas if not already drawing\n",
    "                        canvas = np.zeros_like(frame)\n",
    "\n",
    "                    # Start drawing\n",
    "                    drawing = True\n",
    "\n",
    "                    # Draw line on the canvas\n",
    "                    if start_point is not None:\n",
    "                        cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "                    start_point = (x, y)\n",
    "\n",
    "            else:\n",
    "                # Stop drawing if no hand is detected\n",
    "                drawing = False\n",
    "                start_point = None\n",
    "        else:\n",
    "            # Drawing mode, no hand gesture detection\n",
    "\n",
    "            # Start drawing\n",
    "            if start_point is None:\n",
    "                start_point = (0, 0)\n",
    "\n",
    "            # Get index finger tip coordinates based on mouse movement\n",
    "            x, y = start_point\n",
    "\n",
    "            # Draw line on the canvas\n",
    "            if drawing:\n",
    "                cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "            start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "        \n",
    "        \n",
    "        # Save the drawn image on the blank canvas\n",
    "        blank_canvas[:, :int(WINDOW_WIDTH / 2)] = canvas\n",
    "\n",
    "        # Convert the drawn image to grayscale\n",
    "        grayscale_canvas = cv2.cvtColor(blank_canvas, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Perform any additional operations on the grayscale canvas as needed\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Save the grayscale canvas\n",
    "        cv2.imwrite('drawn_image.jpg', grayscale_canvas)\n",
    "\n",
    "                # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "                # Save the drawn image when 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            # Save the grayscale canvas\n",
    "            cv2.imwrite('drawn_image.jpg', grayscale_canvas)\n",
    "            print(\"Image saved as 'drawn_image.jpg'\")\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "\n",
    "        # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros_like(frame)\n",
    "            print(\"Cleared canvas\")\n",
    "\n",
    "        # Change draw color to red if '1' key is pressed\n",
    "        if key == ord('1'):\n",
    "            draw_color = (0, 0, 255)  # Red\n",
    "            print(\"Switched to red color\")\n",
    "\n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('2'):\n",
    "            draw_color = (0, 255, 0)  # Green\n",
    "            print(\"Switched to green color\")\n",
    "\n",
    "        # Change draw color to blue if '3' key is pressed\n",
    "        if key == ord('3'):\n",
    "            draw_color = (255, 0, 0)  # Blue\n",
    "            print(\"Switched to blue color\")\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf7069",
   "metadata": {},
   "source": [
    "# Hand Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "662badcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Check if hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Draw lines connecting the landmarks for better visualization of hand movement\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                          mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=4),\n",
    "                                          mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                                          )\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Hand Gesture Detection', frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4346d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        if gesture_detection:\n",
    "            # Gesture detection mode\n",
    "\n",
    "            # Check if hand landmarks are detected\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Draw landmarks on the frame\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                              mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                                              mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)\n",
    "                                              )\n",
    "\n",
    "        else:\n",
    "            # Drawing mode, no hand gesture detection\n",
    "\n",
    "            # Get index finger tip coordinates based on mouse movement\n",
    "            if start_point is None:\n",
    "                x, y = 0, 0\n",
    "            else:\n",
    "                x, y = start_point\n",
    "\n",
    "            # Draw line on the canvas\n",
    "            if drawing:\n",
    "                cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "            start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "                    # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Start drawing if 'd' key is pressed\n",
    "        if key == ord('d'):\n",
    "            drawing = True\n",
    "\n",
    "        # Stop drawing if 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            drawing = False\n",
    "\n",
    "# Release the video capture and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6f095",
   "metadata": {},
   "source": [
    "### Showing Landmarks in Gesture mode and able to draw in Drawing mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd03c96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started drawing\n",
      "Started drawing\n",
      "Started drawing\n",
      "Stopped drawing\n",
      "Switched to drawing mode\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Check if hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract hand landmarks and visualize them\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if gesture_detection:\n",
    "                    # Get bounding box coordinates of the hand\n",
    "                    landmarks = hand_landmarks.landmark\n",
    "                    x_values = [landmark.x for landmark in landmarks]\n",
    "                    y_values = [landmark.y for landmark in landmarks]\n",
    "                    min_x = min(x_values) * frame.shape[1]\n",
    "                    max_x = max(x_values) * frame.shape[1]\n",
    "                    min_y = min(y_values) * frame.shape[0]\n",
    "                    max_y = max(y_values) * frame.shape[0]\n",
    "\n",
    "                    # Draw a green rectangle around the hand\n",
    "                    cv2.rectangle(frame, (int(min_x), int(min_y)), (int(max_x), int(max_y)), (0, 255, 0), 2)\n",
    "\n",
    "                    # Start drawing\n",
    "                    if drawing:\n",
    "                        # Get index finger tip coordinates\n",
    "                        index_finger_coords = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                        x, y = int(index_finger_coords.x * frame.shape[1]), int(index_finger_coords.y * frame.shape[0])\n",
    "\n",
    "                        # Draw line on the canvas\n",
    "                        cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "                    start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "\n",
    "        # Start drawing if 'd' key is pressed\n",
    "        if key == ord('d'):\n",
    "            drawing = True\n",
    "            print(\"Started drawing\")\n",
    "\n",
    "        # Stop drawing if 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            drawing = False\n",
    "            print(\"Stopped drawing\")\n",
    "\n",
    "        # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros_like(frame)\n",
    "            print(\"Cleared canvas\")\n",
    "\n",
    "        # Change draw color to red if '1' key is pressed\n",
    "        if key == ord('1'):\n",
    "            draw_color = (0, 0, 255)  # Red\n",
    "            print(\"Switched to red color\")\n",
    "\n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('2'):\n",
    "            draw_color = (0, 255, 0)  # Green\n",
    "            print(\"Switched to green color\")\n",
    "\n",
    "        # Change draw color to blue if '3' key is pressed\n",
    "        if key == ord('3'):\n",
    "            draw_color = (255, 0, 0)  # Blue\n",
    "            print(\"Switched to blue color\")\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e9263f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Switched to drawing mode\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Started drawing\n",
      "Switched to gesture detection mode\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Stopped drawing\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480\n",
    "\n",
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode\n",
    "\n",
    "def save_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the image to (48, 48) dimension\n",
    "    resized = cv2.resize(gray, (48, 48))\n",
    "\n",
    "    # Save the image\n",
    "    cv2.imwrite(\"drawn_image.jpg\", resized)\n",
    "\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Check if hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract hand landmarks and visualize them\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if gesture_detection:\n",
    "                    # Get bounding box coordinates of the hand\n",
    "                    landmarks = hand_landmarks.landmark\n",
    "                    x_values = [landmark.x for landmark in landmarks]\n",
    "                    y_values = [landmark.y for landmark in landmarks]\n",
    "                    min_x = min(x_values) * frame.shape[1]\n",
    "                    max_x = max(x_values) * frame.shape[1]\n",
    "                    min_y = min(y_values) * frame.shape[0]\n",
    "                    max_y = max(y_values) * frame.shape[0]\n",
    "\n",
    "                    # Draw a green rectangle around the hand\n",
    "                    cv2.rectangle(frame, (int(min_x), int(min_y)), (int(max_x), int(max_y)), (0, 255, 0), 2)\n",
    "\n",
    "                    # Start drawing\n",
    "                    if drawing:\n",
    "                        # Get index finger tip coordinates\n",
    "                        index_finger_coords = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                        x, y = int(index_finger_coords.x * frame.shape[1]), int(index_finger_coords.y * frame.shape[0])\n",
    "\n",
    "                        # Draw line on the canvas\n",
    "                        cv2.line(canvas, start_point, (x, y), draw_color, thickness=2)\n",
    "                    start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "\n",
    "        # Start drawing if 'd' key is pressed\n",
    "        if key == ord('d'):\n",
    "            drawing = True\n",
    "            print(\"Started drawing\")\n",
    "\n",
    "        # Stop drawing if 's' key is pressed\n",
    "        if key == ord('s'):\n",
    "            drawing = False\n",
    "            print(\"Stopped drawing\")\n",
    "            save_image(canvas)\n",
    "\n",
    "        # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros_like(frame)\n",
    "            print(\"Cleared canvas\")\n",
    "\n",
    "        # Change draw color to red if '1' key is pressed\n",
    "        if key == ord('1'):\n",
    "            draw_color = (0, 0, 255)  # Red\n",
    "            print(\"Switched to red color\")\n",
    "\n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('2'):\n",
    "            draw_color = (0, 255, 0)  # Green\n",
    "            print(\"Switched to green color\")\n",
    "\n",
    "        # Change draw color to blue if '3' key is pressed\n",
    "        if key == ord('3'):\n",
    "            draw_color = (255, 0, 0)  # Blue\n",
    "            print(\"Switched to blue color\")\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8841dae",
   "metadata": {},
   "source": [
    "# Model that draws and makes predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44010e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from tensorflow import keras\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7a42d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37e40e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for window layout\n",
    "WINDOW_WIDTH = 1280\n",
    "WINDOW_HEIGHT = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "441b4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video capture for webcam\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2825f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the webcam is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "431e852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MediaPipe Hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e263bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing = False\n",
    "start_point = None\n",
    "canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)  # Initialize canvas\n",
    "draw_color = (0, 0, 255)  # Default color: Red (BGR format)\n",
    "gesture_detection = True  # Flag to indicate hand gesture detection mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8edc97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained VGG16-based model\n",
    "model = keras.models.load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e11e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Convert the image to 8-bit per channel or 32-bit float\n",
    "    if image.dtype == np.float64:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "    elif image.dtype == np.float32:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Resize the image to match the input size of the model\n",
    "    image = resize(image, (48, 48), anti_aliasing=True)\n",
    "\n",
    "    # Check if the image is grayscale\n",
    "    if len(image.shape) == 2:\n",
    "        # Convert the grayscale image to RGB using PIL\n",
    "        image = np.stack((image,) * 3, axis=-1)\n",
    "        image = Image.fromarray(image.squeeze(), mode='RGB')\n",
    "    elif image.shape[0] == 1 and image.shape[1] == 1:\n",
    "        # Handle single-pixel image\n",
    "        image = np.tile(image, (48, 48, 3))\n",
    "        image = Image.fromarray(image.squeeze(), mode='RGB')\n",
    "    else:\n",
    "        # Convert the image to RGB using PIL\n",
    "        image = Image.fromarray(image, mode='RGB')\n",
    "\n",
    "    # Convert the image to numpy array\n",
    "    image = np.array(image)\n",
    "\n",
    "    # Normalize the pixel values\n",
    "    image = image.astype('float32') / 255.0\n",
    "\n",
    "    # Reshape the image to match the input shape of the model\n",
    "    image = np.reshape(image, (1, 48, 48, 3))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0e2ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the image to (48, 48) dimension\n",
    "    resized = cv2.resize(gray, (48, 48))\n",
    "\n",
    "    # Save the image\n",
    "    cv2.imwrite(\"drawn_image.jpg\", resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2c55565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input image\n",
    "def preprocess_image(image):\n",
    "    # Resize the image to match the input shape of the model (48x48)\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    # Convert the image to grayscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Convert the grayscale image to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    # Reshape the image to match the input shape of the model (add batch dimension)\n",
    "    image_rgb = np.expand_dims(image_rgb, axis=0)\n",
    "    # Normalize the pixel values\n",
    "    image_rgb = image_rgb.astype('float32') / 255.0\n",
    "    return image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "291ebbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store predicted results\n",
    "predicted_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 3\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 3\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 3\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 3\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted Number: 7\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 3\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 3\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Number: 7\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted Number: 0\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted Number: 3\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Predicted Number: 0\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Number: 7\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 8\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 7\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted Number: 7\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 9\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 7\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted Number: 3\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted Number: 7\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Number: 7\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Cleared canvas\n",
      "Started drawing\n",
      "Cleared canvas\n",
      "Stopped drawing\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted Number: 3\n"
     ]
    }
   ],
   "source": [
    "image_counter = 0\n",
    "while True:\n",
    "    # Read frame from video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Resize the frame to fit the window layout\n",
    "        frame = cv2.resize(frame, (int(WINDOW_WIDTH / 2), WINDOW_HEIGHT))\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Check if hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract hand landmarks and visualize them\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if gesture_detection:\n",
    "                    # Get bounding box coordinates of the hand\n",
    "                    landmarks = hand_landmarks.landmark\n",
    "                    x_values = [landmark.x for landmark in landmarks]\n",
    "                    y_values = [landmark.y for landmark in landmarks]\n",
    "                    min_x = min(x_values) * frame.shape[1]\n",
    "                    max_x = max(x_values) * frame.shape[1]\n",
    "                    min_y = min(y_values) * frame.shape[0]\n",
    "                    max_y = max(y_values) * frame.shape[0]\n",
    "\n",
    "                    # Draw a green rectangle around the hand\n",
    "                    cv2.rectangle(frame, (int(min_x), int(min_y)), (int(max_x), int(max_y)), (0, 255, 0), 2)\n",
    "\n",
    "                    # Start drawing\n",
    "                    if drawing:\n",
    "                        # Get index finger tip coordinates\n",
    "                        index_finger_coords = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                        x, y = int(index_finger_coords.x * frame.shape[1]), int(index_finger_coords.y * frame.shape[0])\n",
    "\n",
    "                        # Draw line on the canvas\n",
    "                        cv2.line(canvas, start_point, (x, y), draw_color, thickness=20)\n",
    "                        start_point = (x, y)\n",
    "\n",
    "        # Create a blank canvas on the right\n",
    "        blank_canvas = np.zeros((WINDOW_HEIGHT, int(WINDOW_WIDTH / 2), 3), dtype=np.uint8)\n",
    "\n",
    "        # Combine the video frame and canvas\n",
    "        output_frame = np.concatenate((frame, canvas), axis=1)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('Air Canvas', output_frame)\n",
    "\n",
    "        # Check for key inputs\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit if 'q' key is pressed\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Toggle between gesture detection and drawing mode if 'm' key is pressed\n",
    "        if key == ord('m'):\n",
    "            gesture_detection = not gesture_detection\n",
    "            if gesture_detection:\n",
    "                print(\"Switched to gesture detection mode\")\n",
    "            else:\n",
    "                print(\"Switched to drawing mode\")\n",
    "\n",
    "        # Start drawing if 'd' key is pressed\n",
    "        if key == ord('d'):\n",
    "            drawing = True\n",
    "            print(\"Started drawing\")\n",
    "            \n",
    "        if key == ord('s'):\n",
    "            drawing = False\n",
    "            print(\"Stopped drawing\")\n",
    "            save_image(canvas)\n",
    "        \n",
    "            # Save the image with a unique name\n",
    "            image_name = '0/drawn_image_' + str(image_counter) + '.jpg'\n",
    "            cv2.imwrite(image_name, canvas)\n",
    "        \n",
    "            # Increment the image counter\n",
    "            image_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Clear the canvas if 'c' key is pressed\n",
    "        if key == ord('c'):\n",
    "            canvas = np.zeros_like(frame)\n",
    "            print(\"Cleared canvas\")\n",
    "\n",
    "                # Change draw color to red if '1' key is pressed\n",
    "        if key == ord('1'):\n",
    "            draw_color = (0, 0, 255)  # Red\n",
    "            print(\"Switched to red color\")\n",
    "\n",
    "        # Change draw color to green if '2' key is pressed\n",
    "        if key == ord('2'):\n",
    "            draw_color = (0, 255, 0)  # Green\n",
    "            print(\"Switched to green color\")\n",
    "\n",
    "        # Change draw color to blue if '3' key is pressed\n",
    "        if key == ord('3'):\n",
    "            draw_color = (255, 0, 0)  # Blue\n",
    "            print(\"Switched to blue color\")\n",
    "            \n",
    "        # Change draw color to blue if '0' key is pressed\n",
    "        if key == ord('0'):\n",
    "            draw_color = (255, 255, 255)  # White\n",
    "            print(\"Switched to White color\")\n",
    "\n",
    "        # Perform prediction if 'g' key is pressed\n",
    "        if key == ord('g'):\n",
    "        # Load and preprocess the input image\n",
    "            image_path = 'drawn_image.jpg'\n",
    "            input_image = cv2.imread(image_path)\n",
    "            preprocessed_image = preprocess_image(input_image)\n",
    "\n",
    "            # Make a prediction\n",
    "            prediction = model.predict(preprocessed_image)\n",
    "            predicted_class = np.argmax(prediction)\n",
    "\n",
    "            # Convert the predicted class to text\n",
    "            predicted_text = str(predicted_class)\n",
    "            print(\"Predicted Number:\", predicted_text)\n",
    "            \n",
    "            # Add the predicted result to the list\n",
    "            predicted_results.append(predicted_text)\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61a122",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365431d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
