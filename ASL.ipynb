{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9fae869",
   "metadata": {},
   "source": [
    "# ASL data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "34b0fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, SpatialDropout2D, Flatten, Dropout\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b0eb1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms as T\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7887e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "from glob import glob\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d107674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class names\n",
    "class_names = [\n",
    "    \"call\", \"dislike\", \"fist\", \"four\", \"like\", \"mute\", \"ok\", \"one\", \"palm\", \n",
    "    \"peace\", \"peace_inverted\", \"rock\", \"stop\", \"stop_inverted\", \"three\", \"three2\", \n",
    "    \"two_up\", \"two_up_inverted\"\n",
    "]\n",
    "FORMATS = (\".jpeg\", \".jpg\", \".jp2\", \".png\", \".tiff\", \".jfif\", \".bmp\", \".webp\", \".heic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "741631ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input directory path\n",
    "data_dir = r\"C:\\Users\\moham\\Documents\\Sem-III\\DAB 322\\subsample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d253e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image dimensions\n",
    "img_height, img_width = 48, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b7b7b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through the folders inside the data directory\n",
    "for class_name in class_names:\n",
    "    folder_path = os.path.join(data_dir, class_name)\n",
    "    \n",
    "    # Iterate through the image files in each folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Load and preprocess the image\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resized = cv2.resize(img, (img_width, img_height))\n",
    "            \n",
    "            \n",
    "            # Append the preprocessed image and its corresponding label to the lists\n",
    "            images.append(img_rgb)\n",
    "            labels.append(class_names.index(class_name))\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c8d7e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1152, 48, 48, 3) (1152,)\n",
      "Validation data shape: (288, 48, 48, 3) (288,)\n",
      "Testing data shape: (360, 48, 48, 3) (360,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, testing, and validation sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training data shape:\", train_images.shape, train_labels.shape)\n",
    "print(\"Validation data shape:\", val_images.shape, val_labels.shape)\n",
    "print(\"Testing data shape:\", test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d57d3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f08e7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on the labels\n",
    "labels_one_hot = to_categorical(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "89470b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "# Define your custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "65ad783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0481eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and data loaders\n",
    "train_dataset = CustomDataset(train_images, train_labels, transform=transform)\n",
    "val_dataset = CustomDataset(val_images, val_labels, transform=transform)\n",
    "test_dataset = CustomDataset(test_images, test_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "87ebcaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "74c0f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the MobileNet model\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "num_ftrs = mobilenet.classifier[1].in_features\n",
    "mobilenet.classifier[1] = nn.Linear(num_ftrs, len(class_names))  # Modify the last layer to match the number of classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# Define your custom CNN architecture\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 24 * 24, 128)  # Update the input size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of your custom CNN\n",
    "custom_cnn = CustomCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f6adb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(custom_cnn.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0cdfc715",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_cnn = custom_cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "27162ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train loss: 3.4809063143200345\n",
      "Val loss: 2.992123391893175, Accuracy: 0.06944444444444445\n",
      "Epoch 2/10\n",
      "Train loss: 3.002975026766459\n",
      "Val loss: 2.988594320085314, Accuracy: 0.06944444444444445\n",
      "Epoch 3/10\n",
      "Train loss: 2.99394592973921\n",
      "Val loss: 2.9851805104149713, Accuracy: 0.06944444444444445\n",
      "Epoch 4/10\n",
      "Train loss: 2.989819096194373\n",
      "Val loss: 2.982100327809652, Accuracy: 0.06944444444444445\n",
      "Epoch 5/10\n",
      "Train loss: 2.9858883950445385\n",
      "Val loss: 2.9792018201616077, Accuracy: 0.06944444444444445\n",
      "Epoch 6/10\n",
      "Train loss: 2.9822345044877796\n",
      "Val loss: 2.9764779143863254, Accuracy: 0.06944444444444445\n",
      "Epoch 7/10\n",
      "Train loss: 2.978741811381446\n",
      "Val loss: 2.9739557372199164, Accuracy: 0.06944444444444445\n",
      "Epoch 8/10\n",
      "Train loss: 2.9754662182595997\n",
      "Val loss: 2.9715692732069225, Accuracy: 0.06944444444444445\n",
      "Epoch 9/10\n",
      "Train loss: 2.9723162055015564\n",
      "Val loss: 2.9693779945373535, Accuracy: 0.06944444444444445\n",
      "Epoch 10/10\n",
      "Train loss: 2.969391405582428\n",
      "Val loss: 2.9672125710381403, Accuracy: 0.06944444444444445\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    custom_cnn.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device, dtype=torch.long) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = custom_cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    print(f\"Train loss: {train_loss}\")\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    custom_cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = custom_cnn(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_dataset)\n",
    "        accuracy = correct / len(val_dataset)\n",
    "        print(f\"Val loss: {val_loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "387fcdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 6.548065047793918, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "custom_cnn.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_dataset)\n",
    "    accuracy = correct / len(test_dataset)\n",
    "    print(f\"Test loss: {test_loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "211c0d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 5.236646334330241, Accuracy: 0.06111111111111111\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()  # Convert labels to Long\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_dataset)\n",
    "    accuracy = correct / len(test_dataset)\n",
    "    print(f\"Test loss: {test_loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "288b7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: one\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = \"C:\\\\Users\\\\moham\\\\Documents\\\\Sem-III\\\\DAB 322\\\\dataset\\\\subsample\\\\dislike\\\\03edcf00-2b0d-4576-bab8-da2bb5da14e2.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
    "image_resized = cv2.resize(image_rgb, (img_width, img_height))\n",
    "image_normalized = image_resized / 255.0  # Normalize the image\n",
    "\n",
    "# Convert the preprocessed image into a PyTorch tensor\n",
    "image_tensor = torch.tensor(image_normalized, dtype=torch.float32)\n",
    "image_tensor = image_tensor.permute(2, 0, 1)  # Adjust the tensor dimensions\n",
    "image_tensor = image_tensor.unsqueeze(0)  # Add an extra dimension for the batch\n",
    "\n",
    "# Pass the image through the model\n",
    "model.eval()\n",
    "image_tensor = image_tensor.to(device)\n",
    "output = model(image_tensor)\n",
    "\n",
    "# Interpret the model's predictions\n",
    "output = output.detach().cpu().numpy()\n",
    "predicted_class_index = np.argmax(output)\n",
    "predicted_class_label = class_names[predicted_class_index]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d4a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3a1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
